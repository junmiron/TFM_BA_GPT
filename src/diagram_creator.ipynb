{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2afcbc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "import asyncio\n",
    "import yaml\n",
    "import chromadb\n",
    "import requests\n",
    "import base64\n",
    "import zlib\n",
    "import json\n",
    "import argparse\n",
    "import tempfile\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from autogen_core.memory import MemoryContent, MemoryMimeType\n",
    "from autogen_core.models import ChatCompletionClient, ModelInfo\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat, Swarm, SelectorGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination, HandoffTermination, TextMentionTermination, MaxMessageTermination\n",
    "from autogen_agentchat.messages import HandoffMessage\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "from autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\n",
    "from autogen_ext.auth.azure import AzureTokenProvider\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader, UnstructuredWordDocumentLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf684a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE = \"C:/Master IA/TFM_BA_GPT/src/model_config_azure.yaml\"\n",
    "OLLAMA = \"C:/Master IA/TFM_BA_GPT/src/model_config_ollama.yaml\"\n",
    "LMSTUDIO = \"C:/Master IA/TFM_BA_GPT/src/model_config_lmstudio.yaml\"\n",
    "\n",
    "# Load model configuration and create the model client.\n",
    "with open(OLLAMA, \"r\") as f:\n",
    "    model_config = yaml.safe_load(f)\n",
    "    \n",
    "model_client = ChatCompletionClient.load_component(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5fbd7",
   "metadata": {},
   "source": [
    "Agent to make diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee8075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a .txt file and returns it as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {e}\"\n",
    "    \n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a .pdf file and returns it as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        return documents[0].page_content if documents else \"\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {e}\"\n",
    "    \n",
    "def write_mermaid_to_file(mermaid_code: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Write Mermaid diagram code to a .mmd file.\n",
    "\n",
    "    Args:\n",
    "        mermaid_code (str): The Mermaid syntax/code to write.\n",
    "        filename (str): The name of the file to write to. Should end with '.mmd'.\n",
    "    \"\"\"\n",
    "    if not filename.endswith('.mmd'):\n",
    "        raise ValueError(\"Filename must end with '.mmd'\")\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"```mermaid\\n\")\n",
    "        file.write(mermaid_code)\n",
    "        file.write(\"\\n```\")\n",
    "\n",
    "    print(f\"Mermaid diagram written to {filename}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23862a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_agent = AssistantAgent(\n",
    "    \"PlanningAgent\",\n",
    "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"\n",
    "    You are a planning agent.\n",
    "    Your job is to break down complex tasks into smaller, manageable subtasks.\n",
    "    Your team members are:\n",
    "    - summarizer: An agent that uses tools to read pdf files and summarizes the process information.\n",
    "    - diagram_creator_agent: An agent that receives the summary of a process and creates diagrams as a code using Mermaid languagge.\n",
    "    - mermaid_code_reviewer_agent: An agent that reviews the Mermaid code from the diagram_creator_agent and provides feedback.\n",
    "\n",
    "    You only plan and delegate tasks - you do not execute them yourself.\n",
    "\n",
    "    When assigning tasks, use this format:\n",
    "    1. <agent> : <task>\n",
    "\n",
    "    After assigning tasks, wait for all agents to finish their tasks.\n",
    "    After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n",
    "    \"\"\",\n",
    "    model_client_stream=True,\n",
    ")\n",
    "\n",
    "diagram_creator_agent = AssistantAgent(\n",
    "    name=\"diagram_creator_agent\",\n",
    "    description=\"An agent that create diagrams as a code.\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"\n",
    "    You are a Process Diagram Creator and an expert in the Mermaid language.\n",
    "    Your job is to create process diagrams as a code using the Mermaid language.\n",
    "    You will receive a summary of the information provided by the summarizer agent.\n",
    "    \"\"\",\n",
    "    model_client_stream=True,\n",
    ")\n",
    "\n",
    "summarizer_agent = AssistantAgent(\n",
    "        name=\"summarizer\",\n",
    "        description=\"An agent that use tools to read pdf files and summarizes the information.\",\n",
    "        model_client=model_client,\n",
    "        tools=[read_pdf_file],\n",
    "        system_message=(\n",
    "            \"You are an expert on making process summaries.\"\n",
    "            \"You will read a pdf file using tools and summarize the information.\"\n",
    "            \"Make sure to include all the important information.\"\n",
    "            \"You do not generate Mermaid code\"\n",
    "        ),\n",
    "        model_client_stream=True,  # Enable model client streaming.\n",
    "    )\n",
    "\n",
    "mermaid_code_reviewer_agent = AssistantAgent(\n",
    "        name=\"mermaid_code_reviewer_agent\",\n",
    "        description=\"An agent that reviews the Mermaid code.\",\n",
    "        model_client=model_client,\n",
    "        tools=[write_mermaid_to_file],\n",
    "        system_message=(\n",
    "            \"You are an expert on the Mermaid languagge.\"\n",
    "            \"Review the code generated by the diagram_creator_agent\"\n",
    "            \"If there is feedback, provide it to the PlanningAgent.\"\n",
    "            \"else use the code with write_mermaid_to_file to save as C:/Master IA/TFM_BA_GPT/doc/mermaid.mmd and end with 'TERMINATE'.\"\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f45dc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
    "max_messages_termination = MaxMessageTermination(max_messages=25)\n",
    "termination = text_mention_termination | max_messages_termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614f917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_prompt = \"\"\"Select an agent to perform task.\n",
    "\n",
    "{roles}\n",
    "\n",
    "Current conversation context:\n",
    "{history}\n",
    "\n",
    "Read the above conversation, then select an agent from {participants} to perform the next task.\n",
    "Make sure the planner agent has assigned tasks before other agents start working.\n",
    "Only select one agent.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7de63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "team = SelectorGroupChat(\n",
    "    [planning_agent, summarizer_agent, diagram_creator_agent, mermaid_code_reviewer_agent],\n",
    "    model_client=model_client,\n",
    "    termination_condition=termination,\n",
    "    selector_prompt=selector_prompt,\n",
    "    allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "752419cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Read the PDF in file_path:'C:/Master IA/TFM_BA_GPT/doc/Sales SOP for acquiring new clients.pdf' file and create a diagram as a code for that process using Mermaid language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9599aabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Read the PDF in file_path:'C:/Master IA/TFM_BA_GPT/doc/Sales SOP for acquiring new clients.pdf' file and create a diagram as a code for that process using Mermaid language\n",
      "---------- PlanningAgent ----------\n",
      "1. summarizer : Read the PDF located at 'C:/Master IA/TFM_BA_GPT/doc/Sales SOP for acquiring new clients.pdf' and provide a summary of the sales process described within.\n",
      "2. diagram_creator_agent : Once you receive the summary from the summarizer, create a diagram as a code using Mermaid language that represents the sales process of acquiring new clients.\n",
      "3. mermaid_code_reviewer_agent : After the diagram is created by the diagram_creator_agent, review the Mermaid code and provide feedback on its accuracy and completeness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for SelectorGroupChatManager_39f6333a-e30a-4d0d-8a5a-c043b5b1e745/39f6333a-e30a-4d0d-8a5a-c043b5b1e745\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 533, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 113, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat_manager.py\", line 189, in handle_agent_response\n",
      "    speaker_name = await speaker_name_future\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_selector_group_chat.py\", line 174, in select_speaker\n",
      "    agent_name = await self._select_speaker(roles, participants, history, self._max_selector_attempts)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_selector_group_chat.py\", line 195, in _select_speaker\n",
      "    response = await self._model_client.create(messages=select_speaker_messages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_ext\\models\\ollama\\_ollama_client.py\", line 619, in create\n",
      "    result: ChatResponse = await future\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\ollama\\_client.py\", line 837, in chat\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<13 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\ollama\\_client.py\", line 682, in _request\n",
      "    return cls(**(await self._request_raw(*args, **kwargs)).json())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\ollama\\_client.py\", line 626, in _request_raw\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: <html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>nginx</center>\n",
      "</body>\n",
      "</html>\n",
      " (status code: 502)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ResponseError: <html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body>\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>nginx</center>\r\n</body>\r\n</html>\r\n (status code: 502)\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat_manager.py\", line 189, in handle_agent_response\n    speaker_name = await speaker_name_future\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_selector_group_chat.py\", line 174, in select_speaker\n    agent_name = await self._select_speaker(roles, participants, history, self._max_selector_attempts)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_selector_group_chat.py\", line 195, in _select_speaker\n    response = await self._model_client.create(messages=select_speaker_messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_ext\\models\\ollama\\_ollama_client.py\", line 619, in create\n    result: ChatResponse = await future\n                           ^^^^^^^^^^^^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\ollama\\_client.py\", line 837, in chat\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n    ...<13 lines>...\n    )\n    ^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\ollama\\_client.py\", line 682, in _request\n    return cls(**(await self._request_raw(*args, **kwargs)).json())\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\ollama\\_client.py\", line 626, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\n\nollama._types.ResponseError: <html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body>\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>nginx</center>\r\n</body>\r\n</html>\r\n (status code: 502)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(team.run_stream(task=task))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\ui\\_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:503\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    500\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    501\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    504\u001b[39m     stop_reason = message.message.content\n\u001b[32m    505\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: ResponseError: <html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body>\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>nginx</center>\r\n</body>\r\n</html>\r\n (status code: 502)\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat_manager.py\", line 189, in handle_agent_response\n    speaker_name = await speaker_name_future\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_selector_group_chat.py\", line 174, in select_speaker\n    agent_name = await self._select_speaker(roles, participants, history, self._max_selector_attempts)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_selector_group_chat.py\", line 195, in _select_speaker\n    response = await self._model_client.create(messages=select_speaker_messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\autogen_ext\\models\\ollama\\_ollama_client.py\", line 619, in create\n    result: ChatResponse = await future\n                           ^^^^^^^^^^^^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\ollama\\_client.py\", line 837, in chat\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n    ...<13 lines>...\n    )\n    ^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\ollama\\_client.py\", line 682, in _request\n    return cls(**(await self._request_raw(*args, **kwargs)).json())\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Master IA\\TFM_BA_GPT\\venv\\Lib\\site-packages\\ollama\\_client.py\", line 626, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\n\nollama._types.ResponseError: <html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body>\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>nginx</center>\r\n</body>\r\n</html>\r\n (status code: 502)\n"
     ]
    }
   ],
   "source": [
    "await Console(team.run_stream(task=task))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
